{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "similar-meditation",
   "metadata": {},
   "source": [
    "# Deep Sets Tutorial\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nhartman94/RTG-2994-ML/blob/main/tutorials/Deep-Sets.ipynb)\n",
    "\n",
    "We were motivated to try Deep Sets ATLAS FTAG from the lovely \"Energy Flow Networks: Deep Sets for Particle Jets\" [paper](https://arxiv.org/abs/1810.05165) which includes a really nice pip installable [energyflow](https://energyflow.network/installation/) package as well.\n",
    "\n",
    "We're using the same notation and variable names as the paper and code base, but this tutorial works with networks built in pytorch so we can understand the network internals.\n",
    "\n",
    "Below are the topics that this tutorial covers:\n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Loading in the dataset](#dataset)\n",
    "    - Q1: Viz the dataset\n",
    "2. [Set up architecture](#model)\n",
    "    - Q2: Implement the `phi` and `F` NNs \n",
    "    - Q3: Implement the Sum operation\n",
    "    - Q4: What's the accuracy of a randomly initialized model?\n",
    "    - Q5: Check your answer to Q4 (in code)\n",
    "3. [Train and evaluate performance](#eval)\n",
    "    - Q6: Evaluate the performance on the validation set\n",
    "    - Q7: Draw losses\n",
    "    - Q8: Implement the discriminant\n",
    "    - Q9: Draw ROC curve\n",
    "    - Q10: Understanding ROC curve (connection w/ Q1)\n",
    "4. [Including layer norm](#ln)\n",
    "    - Q11: Build the model\n",
    "\n",
    "Section (2) & (3) constitutes the bulk of the goals of this tutorial, while section (4) starts to build us up for what comes next in the lecture.\n",
    "\n",
    "Please feel free talk with your neighbor about the open ended questions and implementations in this notebook!\n",
    "\n",
    "6.10.2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b8a2c",
   "metadata": {},
   "source": [
    "## Step 1: Load in a dataset\n",
    "<a name=\"dataset\"></a>\n",
    "\n",
    "For this tutorial, we'll use the $b$-tagging dataseset from J. Shalomi.\n",
    "\n",
    "https://zenodo.org/records/4044628"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-accreditation",
   "metadata": {},
   "source": [
    "I put a subset of these features in an `example.root` to upload the dataset directly to github for ease of iteration. We'll consider 4 features today:\n",
    "\n",
    "$$X_i = \\begin{pmatrix} \n",
    "{d_0} \\\\\n",
    "{z_0} \\\\\n",
    "\\sigma_{d0} \\\\\n",
    "\\sigma_{z0}\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "namely the impact parameters $d_0$ and $z_0$ and their errors.\n",
    "\n",
    "<img src=\"ip.jpg\"\n",
    "     width=500\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c25eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uproot \n",
    "!pip install awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcle phys specific packages -- natively handle variable length ``events''\n",
    "# w/o applying masking\n",
    "import uproot\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasset\n",
    "!wget https://zenodo.org/records/4044628/files/valid_data.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f834dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f\"trk_{v}\" for v in [\"d0\", \"z0\", \"d0err\", \"z0err\"]]\n",
    "cols += [\"jet_flav\"]\n",
    "\n",
    "t = uproot.open(\"valid_data.root:tree\")\n",
    "arr = t.arrays(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0d03e",
   "metadata": {},
   "source": [
    "What are the truth labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f43355",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(arr['jet_flav'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c8833",
   "metadata": {},
   "source": [
    "- 0: light jet\n",
    "- 4: c-jet\n",
    "- 5: b-jet\n",
    "\n",
    "(the latter two numbers corresponding to the pdg IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c161e",
   "metadata": {},
   "source": [
    "**Q1:** Plot the inputs \n",
    "\n",
    "(For starters I show you how to plot $d_0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df078a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip = (-10, 10)\n",
    "kwargs = dict(bins=100, range=clip, histtype=\"step\", density=True, log=True)\n",
    "\n",
    "for yi, label in zip([0,4,5],['light','c','b']):\n",
    "    plt.hist(\n",
    "        ak.flatten(arr['trk_d0'][arr['jet_flav']==yi]),\n",
    "        # color=\"C0\",\n",
    "        label=f\"{label}-jet\",\n",
    "        **kwargs,\n",
    "    )\n",
    "plt.xlabel('trk_d0', fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04768299",
   "metadata": {},
   "source": [
    "Additionally, below I show you the number of tracks / jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ace09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.count(arr['trk_d0'], axis=1), 16, (-.5, 15.5),\n",
    "         histtype=\"step\",density=True)\n",
    "plt.xlabel('Number of tracks / jet')\n",
    "plt.ylabel('Normalized Entries')\n",
    "\n",
    "ylim = plt.ylim()\n",
    "plt.vlines(10.5,*ylim,'k',ls='--')\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92faf2f1",
   "metadata": {},
   "source": [
    "Although the Deep Sets will natively process a naive number of inputs, in practice, we use regular tensors for ease with the GPU.\n",
    "\n",
    "We'll pad jets with less than 10 tracks with fixed dimensional input vectors, and use a mask to ignore these tracks in the computation.\n",
    "\n",
    "I provide you the code for this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~ak.is_none(ak.pad_none(arr[\"trk_d0\"], target=10, clip=True), axis=-1)\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.dstack(\n",
    "    [\n",
    "        ak.where(mask, ak.pad_none(arr[col], target=10, clip=True), 0.0).to_numpy()\n",
    "        for col in cols\n",
    "        if col[:4] == \"trk_\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "X.shape # (181k jets, 10 trks, 4 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0246d21",
   "metadata": {},
   "source": [
    "Now, let's put them in torch tensors... and rename the `y` labels to be 0,1,2 (for convenience with the pytorch functions)\n",
    "\n",
    "- <span style=\"color:royalblue\"> 0 -> **0: light-jets**</span>\n",
    "- <span style=\"color:orange\">4 -> **1 charm-jets**</span> \n",
    "- <span style=\"color:limegreen\"> 5 -> **2: b-jets**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_torch = torch.FloatTensor(X)\n",
    "y_torch = torch.LongTensor(arr['jet_flav'].to_numpy().astype('int'))\n",
    "\n",
    "y_torch[y_torch == 4] = 1\n",
    "y_torch[y_torch == 5] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cce74d",
   "metadata": {},
   "source": [
    "**Train / test split** (following the recommendation from lecture)\n",
    "\n",
    "<img src=\"train_test.jpg\"\n",
    "     width=750\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ec34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77841949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / val / test (80/20/20) split\n",
    "\n",
    "# 20% test data\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X_torch, y_torch, test_size=0.2)\n",
    "\n",
    "# split validation set,\n",
    "# 25% of remaining data goes into the validation set\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-heath",
   "metadata": {},
   "source": [
    "## Step 2: Set up a Deep Sets architecture\n",
    "<a name=\"model\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc2b1e-6422-4d0a-9567-8858041e29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-springer",
   "metadata": {},
   "source": [
    "`pytorch` is very modular and lets you stack layers like legos to build a custom architecture. \n",
    "\n",
    "Your turn! Build the Deep Sets model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dimensions of the input dataset for setting up the model\n",
    "nJets, maxNumTrks, nFeatures = X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04171f16-cc36-42b7-9af9-356f4f0d8fb1",
   "metadata": {},
   "source": [
    "<img src=\"DIPS_architecture_tutorial.jpg\"\n",
    "     width=500\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b3c24-bda7-421f-b331-9bebcf04dc46",
   "metadata": {},
   "source": [
    "**TO DO:** Implement the architecure above.\n",
    "\n",
    "**Q2:** Implement the `phi` and `F` NN (in `__init__`)\n",
    "\n",
    "**Q3:** Implement the `Sum` operation in the `forward` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f63d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nClasses = len(np.unique(y_tr))\n",
    "nClasses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        in_dim = 4\n",
    "\n",
    "        # ppm_sizes_int = [50,50,32]\n",
    "        # dense_sizes_int = [50,50]\n",
    "\n",
    "        # Q2: Implement the phi and F NNs\n",
    "        self.phi = ... # your code here\n",
    "\n",
    "        self.F =  ... # your code here\n",
    "        \n",
    "    def forward(self, x, debug=False):\n",
    "\n",
    "        x = self.phi(x)\n",
    "\n",
    "        if debug: \n",
    "            print('After Phi NN',x.shape)\n",
    "\n",
    "        # Q3: Implement the sum operation\n",
    "        # Hint: Look up torch.sum\n",
    "        x = ... # your code here\n",
    "\n",
    "        if debug:\n",
    "            print('After Sum',x.shape)\n",
    "\n",
    "        x = self.F(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2189263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "dips = DeepSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the output\n",
    "y_out = dips(X_tr,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea243bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little sanity check that implementation has the correct dimensionality\n",
    "assert y_out.shape == (X_tr.shape[0], nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-logistics",
   "metadata": {},
   "source": [
    "As a sanitry check, you can check the trainable layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc799187",
   "metadata": {},
   "outputs": [],
   "source": [
    "dips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-source",
   "metadata": {},
   "source": [
    "### Loss function: Categorical cross entropy\n",
    "\n",
    "To train the model, we mean by a model performing better or worse, which we quantify by a **loss function**. For multi-class classification, we use the categorical cross-entropy, which maximizes the node corresponding to the true label of the jet. \n",
    "\n",
    "For targets $y=[0,1, ..., 3]$, our model is outputting $z \\in \\mathbb{R}^{3}$, the logits (unnormalized probabilities) for these 3 classes.\n",
    "\n",
    "We want to interpret the output of the model probabilistically, which we can do via the softmax:\n",
    "\n",
    "$$\\mathrm{Softmax}(z) \\rightarrow p_i = \\frac{\\exp(z_i)}{\\sum_{i=1}^K\\exp(z_i)}$$\n",
    "\n",
    "The **cross entropy** loss function is then the negative log likelihood of the training data .\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_i \\log p_{y_i},\n",
    "$$\n",
    "\n",
    "where $p_{y_i}$ is the predicted probability of the true target class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-municipality",
   "metadata": {},
   "source": [
    "**Q4:** _Before training_ the model, what do you think the loss will be?\n",
    "\n",
    "\n",
    "- A.   0\n",
    "- B.   $\\infty$\n",
    "- C.   ln(2)\n",
    "- D.   ln(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-thousand",
   "metadata": {},
   "source": [
    "**Q5: Test your answer above in code** \n",
    "\n",
    "Tip: look at `F.cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb769056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.cross_entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda349b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842f798",
   "metadata": {},
   "source": [
    "## Setup 3. Train the model\n",
    "<a name=\"evaluate\"></a>\n",
    "\n",
    "When training a model, we'll iterate over mini-batches of the dataset.\n",
    "\n",
    "The function `get_batch` below takes a sample of 128 jets from the training dataset to get a single update step for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a65926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    '''\n",
    "    Draw a random sample from the training dataset\n",
    "    '''\n",
    "    idx = np.random.choice(len(X_tr),batch_size)\n",
    "    return X_tr[idx], y_tr[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a1806",
   "metadata": {},
   "source": [
    "We give you the code below for training a model in torch (implementing all the pieces we discussed in lecture)\n",
    "- hypothesis class: NN\n",
    "- Data: (x,y) pairs from `get_batch`\n",
    "- Loss: cross entropy\n",
    "- Optimizer: adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - model: pytorch model\n",
    "    - lr: learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f\"training model with {count_parameters(model)} parameters\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    for i in range(200):  # training steps\n",
    "\n",
    "        xi, yi = get_batch(512)  # Draw batch of samples\n",
    "\n",
    "        logits = model(xi)\n",
    "        loss = F.cross_entropy(logits, yi)\n",
    "\n",
    "        opt.zero_grad() # clear the gradients (reset the grad vector to 0)\n",
    "        loss.backward() # Backprob the grad to get dL / dw \n",
    "        opt.step()  # Do the grad update step w = w - lr * (dL / dw )\n",
    "\n",
    "        train_losses.append(float(loss))\n",
    "\n",
    "        with torch.no_grad(): # don't include this in the grad calc\n",
    "\n",
    "            # Q6: Calc and save the performance on the validation set\n",
    "            # (e.g, append answer to val_losses)\n",
    "\n",
    "            # YOUR CODE HERE!\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(float(loss))\n",
    "\n",
    "    return (model, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fde4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dips, train_losses, val_losses = train_model(dips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521ced9",
   "metadata": {},
   "source": [
    "**Q6:** To test your understanding, save the performance on the validation set (modify `train_model` above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d773b3-b575-4cef-8e22-b977dfd7d27c",
   "metadata": {},
   "source": [
    "**Q7:** Plot the training and val loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Q7: YOUR CODE HERE\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9aace",
   "metadata": {},
   "source": [
    "**Intuition building:** Which is lower / higher (train or val loss)? Does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a8ee0-d172-42fb-a3a1-122da2a5304d",
   "metadata": {},
   "source": [
    "Since only 500k jets are being used in the training rn, the model might be overfitting a bit for these hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-microphone",
   "metadata": {},
   "source": [
    "**Evaluate the results with a roc curve**\n",
    "\n",
    "For this multiclass output problem, we'll combine the three class probabilities into a single discriminant.\n",
    "\n",
    "$$D_b = \\log \\frac{p_b}{f_c \\cdot p_c + (1 - f_c) \\cdot p_l}$$\n",
    "\n",
    "where $f_c$ is the \"charm fraction\" which is a parameter we choose after training the network.\n",
    "\n",
    "For this tutorial we will set $f_c = 0.07$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e56335-4585-46e5-8576-addbbade9f0a",
   "metadata": {},
   "source": [
    "**Q7 (warm up):** Will a $b$-jet have high or low values of $D_b$?\n",
    "\n",
    "**Your A:**\n",
    "\n",
    "What about $c$ and $l$-jets?\n",
    "\n",
    "**Your A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39505fc3-181b-47f5-b2f7-97ed6e82a9ec",
   "metadata": {},
   "source": [
    "**Q8: Test your hypo** The code below does the roc curve calculation.\n",
    "\n",
    "TO DO: Fill in the discriminant calculation (the line below starting with)\n",
    "\n",
    "`disc = ...` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2cab1-4ff2-4424-aa39-03ce5148bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigBkgEff(model, X_test, y_test, fc=0.07, title=''):\n",
    "    \"\"\"\n",
    "    Given a model, make the histograms of the model outputs to get the ROC curves.\n",
    "\n",
    "    Input:\n",
    "        model: A pytorch model\n",
    "        X_test: Model inputs for the test set\n",
    "        y_test: Truth labels for the test set\n",
    "        fc: The amount by which to weight the c-jet prob in the disc. The\n",
    "            default value of 0.07 corresponds to the fraction of c-jet bkg\n",
    "            in ttbar.\n",
    "\n",
    "    Output:\n",
    "        effs: A list with 3 entries for the l, c, and b effs\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate the performance with the ROC curves!\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test)\n",
    "\n",
    "    # Pass the output through a Softmax to get the probabilities\n",
    "    probs = F.softmax(logits)\n",
    "\n",
    "    '''\n",
    "    Q8: Set up the discriminant function\n",
    "    '''\n",
    "    disc =  ... # your code here\n",
    "\n",
    "    # Define the min and max range for the distribution\n",
    "    discMax = float(disc.max())\n",
    "    discMin = float(disc.min())\n",
    "    \n",
    "    myRange=(discMin,discMax)\n",
    "    nBins = 200\n",
    "\n",
    "    effs = []\n",
    "    for output, flavor in zip([0,1,2], ['l','c','b']):\n",
    "\n",
    "        ix = (y_test == output)\n",
    "        \n",
    "        # Plot the discriminant output\n",
    "        # nEntries is just a sum of the weight of each bin in the histogram.\n",
    "        nEntries, edges ,_ = plt.hist(disc[ix],alpha=0.5,label=f'{flavor}-jets',\n",
    "                                      bins=nBins, range=myRange, density=True, log=True)\n",
    "        \n",
    "        # Since high Db scores correspond to more b-like jets, compute the cummulative density function\n",
    "        # from summing from high to low values, this is why we reverse the order of the bins in nEntries\n",
    "        # using the \"::-1\" numpy indexing.\n",
    "        eff = np.add.accumulate(nEntries[::-1]) / np.sum(nEntries)\n",
    "        effs.append(eff)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xlabel('$D = \\ln [ p_b / (f_c p_c + (1- f_c)p_l ) ]$',fontsize=14)\n",
    "    plt.ylabel('Normalized entries')\n",
    "        \n",
    "    return effs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'DIPS: $\\Phi$ 50-50-32, F 50-50' \n",
    "\n",
    "leff, ceff, beff = sigBkgEff(dips, X_test, y_test, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed2593",
   "metadata": {},
   "source": [
    "**Q9:** Plot the ROC curve with\n",
    "- b-tagging efficiency on the x axis\n",
    "- light / charm rejection on the y-axis\n",
    "\n",
    "(Recall: rejection = 1/ mis-tag efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q9: your code here\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-bracelet",
   "metadata": {},
   "source": [
    "**Q10:** Which is harder to reject, light or charm jets?\n",
    "\n",
    "Does this make sense given the input distributions you looked at in **Q1**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0a31e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ace61",
   "metadata": {},
   "source": [
    "## Step 4: (Bonus) Layer normalization\n",
    "<a name=\"dataset\"></a>\n",
    "\n",
    "**Goal:** Implement the layer normalization in the Deep Set model\n",
    "\n",
    "Tips:\n",
    "- Look up `nn.LayerNorm`\n",
    "- Put this layer normalization right before the nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet_ln(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        in_dim = 4\n",
    "\n",
    "        self.phi = \n",
    "\n",
    "        self.F = \n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        \n",
    "        x = self.phi(x)\n",
    "        x = torch.sum(x, axis=1)\n",
    "        x = self.F(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dips_ln = DeepSet_ln()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "dips_ln, tr_ln_loss, val_ln_loss = train_model(dips_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, color=\"hotpink\", label=\"training\")\n",
    "plt.plot(val_losses, color=\"hotpink\", label=\"validation\", ls=\"--\")\n",
    "\n",
    "plt.plot(tr_ln_loss, color=\"mediumpurple\", label=\"training\")\n",
    "plt.plot(val_ln_loss, color=\"mediumpurple\", label=\"validation\", ls=\"--\")\n",
    "plt.xlabel(\"Iteration\", fontsize=14)\n",
    "plt.ylabel(\"cross-entropy loss\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.title(\"DIPS with layer norm\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54c118",
   "metadata": {},
   "source": [
    "What's the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "leff_ln, ceff_ln, beff_ln = sigBkgEff(dips_ln, X_test, y_test, title=\"DIPS Layer Norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83023098-7fd8-4fd9-b07b-449a804052ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import pchip\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(6,6),sharex=True,\n",
    "                              gridspec_kw={'height_ratios':[2,1]})\n",
    "\n",
    "for bi,li,ci,c, l in zip([beff,beff_ln],[leff,leff_ln],[ceff,ceff_ln],\n",
    "                         ['hotpink','mediumpurple'],['default','LayerNorm']):\n",
    "\n",
    "    # l-rej\n",
    "    ax1.plot(bi, 1 / li, color=c, label=f'{l}: l-rej')\n",
    "    # c-rej\n",
    "    ax1.plot(bi, 1 / ci, color=c, linestyle='--', label=f'{l}: c-rej')\n",
    "    \n",
    "    \n",
    "# Also the ratio panel\n",
    "xx = np.linspace(0.6,1,101)\n",
    "\n",
    "dx = np.concatenate((np.ones(1),np.diff(beff)))\n",
    "dx_ln = np.concatenate((np.ones(1),np.diff(beff_ln)))\n",
    "\n",
    "# l-rej\n",
    "for bkg,bkg_ln, ls in zip([leff,ceff],[leff_ln,ceff_ln],['-','--']):\n",
    "    \n",
    "    m_num = (bkg_ln!=0) & (dx_ln>0)\n",
    "    m_den = (bkg!=0) & (dx>0)\n",
    "    \n",
    "    f_num = pchip(beff_ln[m_num], 1/bkg_ln[m_num]) \n",
    "    f_den = pchip(beff[m_den],    1/bkg[m_den]) \n",
    "\n",
    "    ax2.plot(xx, f_num(xx) / f_den(xx), ls=ls, color=c)\n",
    "    \n",
    "ax2.set_xlabel('b efficiency')\n",
    "ax1.set_ylabel('Background rejection')\n",
    "ax1.set_ylabel('Background rejection')\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_title(title)\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.set_xlim(0.6,1)\n",
    "ax1.set_ylim(0,int(100))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953192a",
   "metadata": {},
   "source": [
    "And... it is infact helping the performance!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3149fef-6c29-4f29-85ff-d23057fea24b",
   "metadata": {},
   "source": [
    "**Super bonus Q:** Can you plot the activations of the networks with and without layer normalization?\n",
    "\n",
    "(Is layer norm having the desired affect of encouraging activations to zero mean and unit variance?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e93c5c",
   "metadata": {},
   "source": [
    "https://github.com/usatlas-ml-training/Deep-Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7c77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df46bb26-1a4d-432b-be18-fafdeed48f3f",
   "metadata": {},
   "source": [
    "And... that's it for today!!\n",
    "\n",
    "\n",
    "Just as a reminder what we learned in this tutorial:\n",
    "\n",
    "1. Visualizing the dataset\n",
    "2. Building a Deep Sets model for a multi-class classification problem\n",
    "    - Sanity checking... model at initialization\n",
    "3. Training model and evaluating performance\n",
    "4. First foyay into (more on this in the next section!)\n",
    "\n",
    "But the biggest take away is... hopefully you're convinced that Deep Sets is a fast and easy model to get running - and inspired to test it out on some of your LHC research problems :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda4304",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (vsc-nb312)",
   "language": "python",
   "name": "vsc-nb312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
